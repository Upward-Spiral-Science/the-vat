{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis Round 2\n",
    "\n",
    "Now that we have looked at the data on an unfiltered way, we now take into account information we know about the data. For example, the channels, channel types, proteins we know.\n",
    "\n",
    "Reference: http://www.nature.com/articles/sdata201446\n",
    "\n",
    "Co-localization:\n",
    "\"Examining the cross-correlations at small 2d shifts between images reveals that pairs of antibodies which are expected to colocalize within either pre- or postsynaptic compartments (for example, Synapsin1 and vGluT1 or PSD95 and GluR2, respectively) have sharp peaks of correlation, while pairs of antibodies which represent associated pre- and postsynaptic compartments (for example, Synapsin1 and PSD95) have broader, more diffuse cross-correlation peaks\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import os, csv, json\n",
    "\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.measure\n",
    "\n",
    "# pretty charting\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Synap_01', 'Synap_02', 'VGlut1_01', 'VGlut1_02', 'VGlut2', 'Vglut3', 'psd', 'glur2', 'nmdar1', 'nr2b', 'gad', 'VGAT', 'PV', 'Gephyr', 'GABAR1', 'GABABR', 'CR1', '5HT1A', 'NOS', 'TH', 'VACht', 'Synapo', 'tubuli', 'DAPI']\n",
      "['ex.pre', 'ex.pre', 'ex.pre', 'ex.pre', 'ex.pre', 'in.pre.small', 'ex.post', 'ex.post', 'ex.post', 'ex.post', 'in.pre', 'in.pre', 'in.pre', 'in.post', 'in.post', 'in.post', 'in.pre.small', 'other', 'ex.post', 'other', 'other', 'ex.post', 'none', 'none']\n"
     ]
    }
   ],
   "source": [
    "# channel = ['Synap','Synap','VGlut1','VGlut1','VGlut2','Vglut3',\n",
    "#            'psd','glur2','nmdar1','nr2b','gad','VGAT', 'PV','Gephyr',\n",
    "#            'GABAR1','GABABR','CR1','5HT1A', 'NOS','TH','VACht',\n",
    "#            'Synapo','tubuli','DAPI']\n",
    "\n",
    "channel = ['Synap_01','Synap_02','VGlut1_01','VGlut1_02','VGlut2','Vglut3',\n",
    "           'psd','glur2','nmdar1','nr2b','gad','VGAT', 'PV','Gephyr',\n",
    "           'GABAR1','GABABR','CR1','5HT1A', 'NOS','TH','VACht',\n",
    "           'Synapo','tubuli','DAPI']\n",
    "\n",
    "channeltype = ['ex.pre','ex.pre','ex.pre','ex.pre','ex.pre','in.pre.small', \n",
    "               'ex.post','ex.post','ex.post','ex.post','in.pre','in.pre', \n",
    "               'in.pre','in.post','in.post','in.post','in.pre.small','other',\n",
    "               'ex.post','other','other','ex.post','none','none']\n",
    "print channel\n",
    "print channeltype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in volume data\n",
    "list_of_locations = []\n",
    "with open('data/synapsinR_7thA.tif.Pivots.txt') as file:\n",
    "    for line in file:\n",
    "        inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "        \n",
    "        # create list of features\n",
    "        list_of_locations.append(inner_list)\n",
    "\n",
    "# conver to a numpy matrix\n",
    "list_of_locations = np.array(list_of_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### RUN AT BEGINNING AND TRY NOT TO RUN AGAIN - TAKES WAY TOO LONG ####\n",
    "# write new list_of_features to new txt file\n",
    "csvfile = \"data_normalized/shortenedFeatures_normalized.txt\"\n",
    "\n",
    "# load in the feature data\n",
    "list_of_features = []\n",
    "with open(csvfile) as file:\n",
    "    for line in file:\n",
    "        inner_list = [float(elt.strip()) for elt in line.split(',')]\n",
    "        \n",
    "        # create list of features\n",
    "        list_of_features.append(inner_list)\n",
    "\n",
    "# conver to a numpy matrix\n",
    "list_of_features = np.array(list_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0   1513.0\n",
      "23.0   12980.0\n",
      "2.0   40.0\n",
      "1485.0\n",
      "12957.0\n",
      "38.0\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, len(list_of_locations)):\n",
    "print min(list_of_locations[:,0]), \" \", max(list_of_locations[:,0])\n",
    "print min(list_of_locations[:,1]), \" \", max(list_of_locations[:,1])\n",
    "print min(list_of_locations[:,2]), \" \", max(list_of_locations[:,2])\n",
    "\n",
    "print abs(min(list_of_locations[:,0]) - max(list_of_locations[:,0]))\n",
    "print abs(min(list_of_locations[:,1]) - max(list_of_locations[:,1]))\n",
    "print abs(min(list_of_locations[:,2]) - max(list_of_locations[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of protein expressions are:\n",
      "This number should be 24:  24\n"
     ]
    }
   ],
   "source": [
    "# Make a feature dictionary for all the different protein expressions\n",
    "features = {}\n",
    "for idx, chan in enumerate(channel):\n",
    "    indices = [0+idx, 24+idx, 48+idx, 72+idx]\n",
    "    features[chan] = list_of_features[:,indices]\n",
    "    \n",
    "print \"The number of protein expressions are:\"\n",
    "print \"This number should be 24: \", len(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique channel types are:  7\n",
      "['ex.post' 'ex.pre' 'in.post' 'in.pre' 'in.pre.small' 'none' 'other']\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print \"The number of unique channel types are: \", len(np.unique(channeltype))\n",
    "print np.unique(channeltype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
